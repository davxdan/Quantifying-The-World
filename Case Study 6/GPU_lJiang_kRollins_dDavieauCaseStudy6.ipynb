{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test to Ensure GPU Functional and Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "from  IPython import display\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import pathlib\n",
    "import shutil\n",
    "import tempfile\n",
    "print(tf.__version__)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DD:__ Check to see if CUDA GPU is available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-c8366cefad0a>:2: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available(\n",
    "    cuda_only=True, min_cuda_compute_capability=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DD:__ Try new function based on warnings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DD:__ I was wondering how to make sure GPU does the processing rather than CPU. I found the following excerpt from Tensorflow documentation:  \n",
    "\n",
    "\"If a TensorFlow operation has both CPU and GPU implementations, by default the GPU devices will be given priority when the operation is assigned to a device. For example, tf.matmul has both CPU and GPU kernels. On a system with devices CPU:0 and GPU:0, the GPU:0 device will be selected to run tf.matmul unless you explicitly request running it on another device.\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.getcwd()\n",
    "#os.chdir('C:\\\\Users\\\\danie\\\\Documents\\\\GitHub\\\\Quantifying-The-World\\\\Case Study 6\\\\data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DD:__ To get the tensorflow docs packages working I had to clone the github repository to my local file system and run:  \n",
    "_pip install -q C:\\Users\\danie\\Documents\\GitHub\\docs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data\n",
    "gz = tf.keras.utils.get_file('HIGGS.csv.gz', 'https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz')\n",
    "FEATURES = 28\n",
    "ds = tf.data.experimental.CsvDataset(gz,[float(),]*(FEATURES+1), compression_type=\"GZIP\")\n",
    "\n",
    "def pack_row(*row):\n",
    "  label = row[0]\n",
    "  features = tf.stack(row[1:],1)\n",
    "  return features, label\n",
    "\n",
    "packed_ds = ds.batch(10000).map(pack_row).unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[ 0.8692932  -0.6350818   0.22569026  0.32747006 -0.6899932   0.75420225\n",
      " -0.24857314 -1.0920639   0.          1.3749921  -0.6536742   0.9303491\n",
      "  1.1074361   1.1389043  -1.5781983  -1.0469854   0.          0.65792954\n",
      " -0.01045457 -0.04576717  3.1019614   1.35376     0.9795631   0.97807616\n",
      "  0.92000484  0.72165745  0.98875093  0.87667835], shape=(28,), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for features,label in packed_ds.batch(1000).take(1):\n",
    "  print(features[0])\n",
    "  plt.hist(features.numpy().flatten(), bins = 101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DD:__ This is where we will tweek to mimic the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"parameters were chosen using a subset of the HIGGS data \n",
    "#consisting of 2.6 million training examples and 100,000\n",
    "#validation examples.\"\n",
    "\n",
    "N_VALIDATION = int(100)\n",
    "N_TRAIN = int(300)\n",
    "BUFFER_SIZE = int(300)\n",
    "BATCH_SIZE = 300\n",
    "STEPS_PER_EPOCH = N_TRAIN//BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_ds = packed_ds.take(N_VALIDATION).cache()\n",
    "train_ds = packed_ds.skip(N_VALIDATION).take(N_TRAIN).cache()\n",
    "validate_ds = validate_ds.batch(BATCH_SIZE)\n",
    "train_ds = train_ds.shuffle(BUFFER_SIZE).repeat().batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DD:__ Original study's code appears to be using a multilayer perceptron \".MLP\" as per pylearn documentation.  \n",
    "\n",
    "According to Tensorflow documentation Sequential() is the equivalent.  \n",
    "\n",
    "_\"To build a simple, fully-connected network (i.e. multi-layer perceptron): model = tf.keras.Sequential()\"_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 29        \n",
      "=================================================================\n",
      "Total params: 841\n",
      "Trainable params: 841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#\"We selected a ﬁve-layer neural network with 300 hidden units\n",
    "#in each layer, a learning rate of 0.05, and a weight decay \n",
    "#coeﬃcient of 1×10−5. \n",
    "\n",
    "#\"Hidden units all used the tanh activation function.\"\n",
    "\n",
    "#Weights were initialized from a normal distribution with\n",
    "#zero mean and standard deviation 0.1 in the ﬁrst layer,\n",
    "#0.001 in the output layer, and 0.05 all other hidden layers. \n",
    "\n",
    "#\"Gradient computations were made on mini-batches of size 100.\n",
    "#A momentum term increased linearly over the ﬁrst 200 epochs \n",
    "#from 0.9 to 0.99, at which point it remained constant.\"\n",
    "\n",
    "#\"The learning rate decayed by a factor of 1.0000002 every\n",
    "#batch update until it reached a minimum of 10−6\"\n",
    "\n",
    "#\"Training ended when the momentum had reached its maximum\n",
    "#value and the minimum error on the validation set (500,000\n",
    "#examples) had not decreased by more than a factor of \n",
    "#0.00001 over 10 epochs. This early stopping prevented \n",
    "#overﬁtting and resulted in each neural network being trained\n",
    "#for 200-1000 epochs.\"\n",
    "\n",
    "# 'Computations were performed using machines with 16 Intel Xeon\n",
    "# 'cores, an NVIDIA Tesla C2070 graphics processor, and 64 GB \n",
    "# 'memory. All neural networks were trained using the GPU-accelerated\n",
    "# 'Theano and Pylearn2 software libraries [24, 25]. Our code is \n",
    "# 'available at https://github.com/uci-igb/higgs-susy.\n",
    "\n",
    "\n",
    "reproduced_model = tf.keras.Sequential([\n",
    "    layers.Dense(28, activation='tanh', input_shape=(FEATURES,)),\n",
    "#    layers.Dense(300, activation='tanh', input_shape=(FEATURES,)),\n",
    "#    layers.Dense(300, activation='tanh'),\n",
    "#    layers.Dense(300, activation='tanh'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "reproduced_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DD:__ Code below is for reference. It is from the Github described in the article.  \n",
    "\n",
    "__Script for training NN on HIGGS dataset.__  \n",
    "Peter Sadowski  \n",
    "Simply specify the parameters in the 'main' function. Then run in python.  \n",
    "This script will generate two files with a common basename generated from the parameters.  \n",
    "\n",
    "import sys\n",
    "import os\n",
    "import theano\n",
    "import pylearn2\n",
    "import pylearn2.models.mlp as mlp\n",
    "import pylearn2.training_algorithms\n",
    "import pylearn2.training_algorithms.sgd\n",
    "import pylearn2.costs\n",
    "import pylearn2.train\n",
    "import pylearn2.termination_criteria\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "\n",
    "#from hyperopt import STATUS_OK, STATUS_FAIL\n",
    "import pylearn2.datasets.physics\n",
    "def init_train(args):\n",
    "    # Interpret arguments.\n",
    "    derived_feat, seed, nhid, width, lrinit, lrdecay, momentum_init, momentum_saturate, wdecay, dropout_include, = args\n",
    "    derived_feat = derived_feat # string\n",
    "    seed = int(seed)\n",
    "    nhid = int(nhid)\n",
    "    width = int(width)\n",
    "    lrinit = float(lrinit)\n",
    "    lrdecay = float(lrdecay)\n",
    "    momentum_init = float(momentum_init)\n",
    "    momentum_saturate = int(momentum_saturate)\n",
    "    wdecay = float(wdecay)\n",
    "    dropout_include = float(dropout_include) # dropout include probability in top layer.\n",
    "\n",
    "    # Specify output files: log and saved model pkl.\n",
    "    #idpath = os.path.splitext(os.path.abspath(__file__))[0] # ID for output files.\n",
    "\n",
    "    idpath = ''\n",
    "    idpath = idpath + '%s_%d_%d_%d_%0.5f_%0.9f_%0.2f_%d_%f_%0.1f' %(derived_feat, seed, nhid, width, lrinit, lrdecay, momentum_init, momentum_saturate, wdecay, dropout_include) \n",
    "    save_path = idpath + '.pkl'\n",
    "    logfile = idpath + '.log'\n",
    "    print 'Using=%s' % theano.config.device # Can use gpus. \n",
    "    print 'Writing to %s' % logfile\n",
    "    print 'Writing to %s' % save_path\n",
    "    sys.stdout = open(logfile, 'w')\n",
    "\n",
    "    # Dataset\n",
    "    benchmark = 1\n",
    "    dataset_train = pylearn2.datasets.physics.PHYSICS(which_set='train', benchmark=benchmark, derived_feat=derived_feat)\n",
    "    #dataset_train = pylearn2.datasets.physics.PHYSICS(which_set='train', benchmark=benchmark, derived_feat=derived_feat, start=0, stop=2600000) # Smaller set for choosing hyperparameters.\n",
    "    dataset_train_monitor = pylearn2.datasets.physics.PHYSICS(which_set='train', benchmark=benchmark, derived_feat=derived_feat, start=0,stop=100000)\n",
    "    dataset_valid = pylearn2.datasets.physics.PHYSICS(which_set='valid', benchmark=benchmark, derived_feat=derived_feat)\n",
    "    dataset_test = pylearn2.datasets.physics.PHYSICS(which_set='test', benchmark=benchmark, derived_feat=derived_feat)\n",
    "\n",
    "    # Model\n",
    "    nvis = dataset_train.X.shape[1]\n",
    "    istdev = 1.0/np.sqrt(width)\n",
    "    layers = []\n",
    "    for i in range(nhid):\n",
    "        # Hidden layer i\n",
    "        layer = mlp.Tanh(layer_name = 'h%d' % i, dim=width,\n",
    "                        istdev = (istdev if i>0 else 0.1), # First layer should have higher stdev.\n",
    "                        )\n",
    "        layers.append(layer)\n",
    "    #layers.append(mlp.Sigmoid(layer_name='y', dim=1, istdev=istdev/100.0))\n",
    "    layers.append(mlp.Sigmoid(layer_name='y', dim=1, istdev=0.001))\n",
    "    model = pylearn2.models.mlp.MLP(layers, nvis=nvis, seed=seed)\n",
    "\n",
    "    # Cost\n",
    "    cost = pylearn2.costs.mlp.Default() # Default cost.\n",
    "    if dropout_include != 1.0:\n",
    "        # Use dropout cost if specified.\n",
    "        cost = pylearn2.costs.mlp.dropout.Dropout(\n",
    "                        input_include_probs={'y': dropout_include},\n",
    "                        input_scales={'y': 1.0/dropout_include},\n",
    "                        default_input_include_prob = 1.0,\n",
    "                        default_input_scale = 1.0)\n",
    "\n",
    "    if wdecay != 0.0:\n",
    "        # Add weight decay term if specified.\n",
    "        cost2 = pylearn2.costs.mlp.WeightDecay(coeffs = [wdecay]*(nhid+1)) # wdecay if specified.\n",
    "        cost = pylearn2.costs.cost.SumOfCosts(costs=[cost, cost2])\n",
    "\n",
    "# Algorithm\n",
    "    algorithm = pylearn2.training_algorithms.sgd.SGD(\n",
    "                    batch_size=100,   # If changed, change learning rate!\n",
    "                    learning_rate = lrinit, #.05, # In dropout paper=10 for gradient averaged over batch. Depends on batchsize.\n",
    "                    learning_rule = pylearn2.training_algorithms.learning_rule.Momentum(\n",
    "                                        init_momentum = momentum_init,\n",
    "                                    ),\n",
    "\n",
    "                    monitoring_dataset = {'train':dataset_train_monitor,\n",
    "                                          'valid':dataset_valid,\n",
    "                                          'test':dataset_test\n",
    "                                          },\n",
    "\n",
    "                    termination_criterion=pylearn2.termination_criteria.Or(criteria=[\n",
    "                                            pylearn2.termination_criteria.MonitorBased(\n",
    "                                                channel_name=\"valid_y_kl\",\n",
    "                                                prop_decrease=0.00001,\n",
    "                                                N=10),\n",
    "\n",
    "                                            pylearn2.termination_criteria.EpochCounter(\n",
    "                                                max_epochs=momentum_saturate)\n",
    "                                            ]),\n",
    "                    cost=cost,\n",
    "                    update_callbacks=pylearn2.training_algorithms.sgd.ExponentialDecay(\n",
    "                                        decay_factor = lrdecay, #1.0000002 # Decreases by this factor every batch. (1/(1.000001^8000)^100 \n",
    "                                        min_lr=.000001\n",
    "                                        )\n",
    "                )\n",
    "\n",
    "    # Extensions \n",
    "\n",
    "    extensions=[ \n",
    "        #pylearn2.train_extensions.best_params.MonitorBasedSaveBest(channel_name='train_y_misclass',save_path=save_path)\n",
    "        pylearn2.training_algorithms.learning_rule.MomentumAdjustor(\n",
    "            start=0,\n",
    "            saturate = momentum_saturate, # 200,500\n",
    "            final_momentum = 0.99,  # Dropout=.5->.99 over 500 epochs.\n",
    "            )\n",
    "        ]\n",
    "    # Train\n",
    "    train = pylearn2.train.Train(dataset=dataset_train,\n",
    "                                 model=model,\n",
    "                                 algorithm=algorithm,\n",
    "                                 extensions=extensions,\n",
    "                                 save_path=save_path,\n",
    "                                 save_freq=100)\n",
    "\n",
    "    return train\n",
    "\n",
    " \n",
    "def compute_objective(args):\n",
    "    '''Initializes neural network with specified arguments and trains.'''    \n",
    "    # Train network.\n",
    "    train = init_train(args)\n",
    "    train.main_loop()\n",
    "    model = train.model\n",
    "     \n",
    "    # Return objective to hyperopt.\n",
    "    loss = train.model.monitor.channels['valid_y_kl'].val_record[-1]\n",
    "    return loss\n",
    "    #return {\"loss\":loss,\"status\":STATUS_OK}\n",
    "\n",
    "if __name__=='__main__':\n",
    "    # Initialize and train.\n",
    "    #args = (derived_feat, seed, nhid, width, lrinit, lrdecay, momentum_init, momentum_saturate, wdecay, dropout_include)\n",
    "    args = ('True', 42, 1, 1000, 0.0005, 1.0000005, 0.9, 200, 0.00000, 1.0)\n",
    "    compute_objective(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "reproduced_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.05), loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.losses.BinaryCrossentropy(from_logits=True,name='binary_crossentropy')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2084e6d66c8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reproduced_model.fit(train_ds,\n",
    "    steps_per_epoch = STEPS_PER_EPOCH,\n",
    "    epochs=1000,\n",
    "    validation_data=validate_ds,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1/Unknown - 0s 25ms/step - loss: 1.1620 - binary_crossentropy: 1.1620"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1619526147842407, 1.1619526]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reproduced_model.evaluate(validate_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
