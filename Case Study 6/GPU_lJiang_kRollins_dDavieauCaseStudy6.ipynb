{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test to Ensure GPU Functional and Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "from  IPython import display\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import pathlib\n",
    "import shutil\n",
    "import tempfile\n",
    "from IPython.display import Image\n",
    "import time\n",
    "print(tf.__version__)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DD:__ Check to see if CUDA GPU is available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.is_gpu_available(\n",
    "    cuda_only=True, min_cuda_compute_capability=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DD:__ Try new function based on warnings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DD:__ I was wondering how to make sure GPU does the processing rather than CPU. I found the following excerpt from Tensorflow documentation:  \n",
    "\n",
    "_\"If a TensorFlow operation has both CPU and GPU implementations, by default the GPU devices will be given priority when the operation is assigned to a device. For example, tf.matmul has both CPU and GPU kernels. On a system with devices CPU:0 and GPU:0, the GPU:0 device will be selected to run tf.matmul unless you explicitly request running it on another device.\"_  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DD:__ To get the tensorflow docs packages working I had to clone the github repository to my local file system and run:  \n",
    "_pip install -q C:\\Users\\danie\\Documents\\GitHub\\docs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data\n",
    "gz = tf.keras.utils.get_file('HIGGS.csv.gz', 'https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz')\n",
    "FEATURES = 28\n",
    "ds = tf.data.experimental.CsvDataset(gz,[float(),]*(FEATURES+1), compression_type=\"GZIP\")\n",
    "\n",
    "def pack_row(*row):\n",
    "  label = row[0]\n",
    "  features = tf.stack(row[1:],1)\n",
    "  return features, label\n",
    "\n",
    "packed_ds = ds.batch(10000).map(pack_row).unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for features,label in packed_ds.batch(1000).take(1):\n",
    "  print(features[0])\n",
    "  plt.hist(features.numpy().flatten(), bins = 101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DD:__ This is where we will tweek to mimic the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"parameters were chosen using a subset of the HIGGS data \n",
    "#consisting of 2.6 million training examples and 100,000\n",
    "#validation examples.\"\n",
    "\n",
    "N_VALIDATION = int(100000)\n",
    "N_TRAIN = int(2500000)\n",
    "BUFFER_SIZE = int(2500000)\n",
    "BATCH_SIZE = 1000\n",
    "STEPS_PER_EPOCH = N_TRAIN//BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_ds = packed_ds.take(N_VALIDATION).cache()\n",
    "train_ds = packed_ds.skip(N_VALIDATION).take(N_TRAIN).cache()\n",
    "validate_ds = validate_ds.batch(BATCH_SIZE)\n",
    "train_ds = train_ds.shuffle(BUFFER_SIZE).repeat().batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DD:__ Original study's code has \".MLP\"; which indicates they are using a multilayer perceptron as per pylearn documentation.  \n",
    "\n",
    "According to Tensorflow documentation Sequential() is the equivalent to a multilayer perceptron.  \n",
    "\n",
    "_\"To build a simple, fully-connected network (i.e. multi-layer perceptron): model = tf.keras.Sequential()\"_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DD:__ Excerpts from the paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"We selected a ﬁve-layer neural network with 300 hidden units\n",
    "#in each layer, a learning rate of 0.05, and a weight decay \n",
    "#coeﬃcient of 1×10−5. \n",
    "\n",
    "#\"Hidden units all used the tanh activation function.\"\n",
    "\n",
    "#Weights were initialized from a normal distribution with\n",
    "#zero mean and standard deviation 0.1 in the ﬁrst layer,\n",
    "#0.001 in the output layer, and 0.05 all other hidden layers. \n",
    "\n",
    "#\"Gradient computations were made on mini-batches of size 100.\n",
    "#A momentum term increased linearly over the ﬁrst 200 epochs \n",
    "#from 0.9 to 0.99, at which point it remained constant.\"\n",
    "\n",
    "#\"The learning rate decayed by a factor of 1.0000002 every\n",
    "#batch update until it reached a minimum of 10−6\"\n",
    "\n",
    "#\"Training ended when the momentum had reached its maximum\n",
    "#value and the minimum error on the validation set (500,000\n",
    "#examples) had not decreased by more than a factor of \n",
    "#0.00001 over 10 epochs. This early stopping prevented \n",
    "#overﬁtting and resulted in each neural network being trained\n",
    "#for 200-1000 epochs.\"\n",
    "\n",
    "# 'Computations were performed using machines with 16 Intel Xeon\n",
    "# 'cores, an NVIDIA Tesla C2070 graphics processor, and 64 GB \n",
    "# 'memory. All neural networks were trained using the GPU-accelerated\n",
    "# 'Theano and Pylearn2 software libraries [24, 25]. Our code is \n",
    "# 'available at https://github.com/uci-igb/higgs-susy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DD:__ For reference: [higgs/analysis/template.py](higgs/analysis/template.py) is code from the Github described in the article. I used it to verify some of the parameters.\n",
    "\n",
    "The article explains that they used _\"Weight Decay 10^-5\"_ which is equivalent to L2 regularization in tensorflow hence we have __kernel_regularizer=regularizers.l2(0.00001))__ in our code.\n",
    "\n",
    "_L2 regularization is also called weight decay in the context of neural networks_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DD__ We set __patience=10__ in call backs to mirror the original i.e. _\"minimum error on the validation set (500,000 #examples) had not decreased by more than a factor of #0.00001 over 10 epochs\"_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reproduced_model = tf.keras.Sequential([\n",
    "    layers.Dense(300, activation='tanh',\n",
    "                 kernel_regularizer=regularizers.l2(0.00001),\n",
    "                 input_shape=(FEATURES,)),\n",
    "    layers.Dense(300, activation='tanh'),\n",
    "    layers.Dense(300, activation='tanh'),\n",
    "    layers.Dense(300, activation='tanh'),\n",
    "   # layers.Dense(300, activation='tanh'),\n",
    "   # Using 5 layers gave us 370,201 parameters. \n",
    "   # I suspect that the paper is counting the input layer therefore they actually only used 4 so we only use 4\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='binary_crossentropy', patience=10)\n",
    "\n",
    "reproduced_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.05), loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.losses.BinaryCrossentropy(from_logits=True,name='binary_crossentropy'),\n",
    "                  'accuracy'], run_eagerly=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DD:__ Using 5 layers gave us 370,201 parameters.  \n",
    "__I suspect that the paper is counting the input layer therefore they actually only used 4.__\n",
    "\n",
    "_The largest shallow network had 300,001 parameters, slightly more than the 279,901 parameters in the largest deep network_\n",
    "\n",
    "Setting us up with 4 layers yields 279,901 parameters. Here is our reproduced the model; as best we can given the information available in the paper and the time that we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reproduced_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "reproduced_model.fit(train_ds,\n",
    "    steps_per_epoch = STEPS_PER_EPOCH,\n",
    "    epochs=10000,\n",
    "    callbacks=[callback],\n",
    "    validation_data=validate_ds,\n",
    "    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"GPU_Activity.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Image(\"CPU_Activity.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DD__: Here are our results;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reproduced_model.evaluate(validate_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DD:__ Model took approximately 3 minutes to run. Results are terrible in comparison to the original study. This is likely due to:  \n",
    "\n",
    "- We did notexplicitly replicate the weights used in the originial study\n",
    "- We are using tensorflow 2.1.0, the original used Theano and Pylearn2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result=reproduced_model(features)\n",
    "# regularization_loss=tf.add_n(reproduced_model.losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DD:__ Given functional more powerful hardware than was available in 2014 we could attempt a gridsearch to determine number of layers.  \n",
    "Here is excerpt of code that uses a grid search from https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original paper's hardware specs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"OriginalHardware.PNG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our hardware specs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"OurHardware.PNG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DD:__ Try adjusting parameters based on tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_model_shallow = tf.keras.Sequential([\n",
    "    layers.Dense(300, activation='elu',\n",
    "                 kernel_regularizer=regularizers.l2(0.001),\n",
    "                 input_shape=(FEATURES,)),\n",
    "    layers.Dropout(0.1),\n",
    "#     layers.Dense(300, activation='elu'),\n",
    "#     layers.Dropout(0.5),\n",
    "#     layers.Dense(300, activation='elu'),\n",
    "#     layers.Dropout(0.5),\n",
    "#     layers.Dense(300, activation='elu'),\n",
    "#     layers.Dropout(0.5),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# adjusted_model = tf.keras.Sequential([\n",
    "#     layers.Dense(300, activation='elu',\n",
    "#                  kernel_regularizer=regularizers.l2(0.001),\n",
    "#                  input_shape=(FEATURES,)),\n",
    "#     layers.Dense(300, activation='elu'),\n",
    "#     layers.Dropout(.1),\n",
    "#     layers.Dense(300, activation='elu'),\n",
    "#     layers.Dropout(.1, activation='elu'),\n",
    "#     layers.Dense(300, activation='elu'),\n",
    "#     layers.Dropout(.1, activation='elu'),\n",
    "#    # layers.Dense(300, activation='tanh'),\n",
    "#    # Using 5 layers gave us 370,201 parameters. \n",
    "#    # I suspect that the paper is counting the input layer therefore they actually only used 4 so we only use 4\n",
    "#     layers.Dense(1)\n",
    "# ])\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='binary_crossentropy', patience=10)\n",
    "\n",
    "adjusted_model_shallow.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.05), loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.losses.BinaryCrossentropy(from_logits=True,name='binary_crossentropy'),\n",
    "                  'accuracy'], run_eagerly=False)\n",
    "\n",
    "adjusted_model_shallow.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "adjusted_model_shallow.fit(train_ds,\n",
    "    steps_per_epoch = STEPS_PER_EPOCH,\n",
    "    epochs=10000,\n",
    "    callbacks=[callback],\n",
    "    validation_data=validate_ds,\n",
    "    verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DD__: With only 1 layer we have significant improvement  but still only 70% accurate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adjusted_model_shallow.evaluate(validate_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DD__: Lets try same as above but deep with 5 layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_model_deep5 = tf.keras.Sequential([\n",
    "    layers.Dense(300, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(0.001),\n",
    "                 input_shape=(FEATURES,)),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(300, activation='relu'),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(300, activation='relu'),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(300, activation='relu'),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='binary_crossentropy', patience=10)\n",
    "\n",
    "adjusted_model_deep5.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.05), loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.losses.BinaryCrossentropy(from_logits=True,name='binary_crossentropy'),\n",
    "                  'accuracy'], run_eagerly=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_model_deep5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "adjusted_model_deep5.fit(train_ds,\n",
    "    steps_per_epoch = STEPS_PER_EPOCH,\n",
    "    epochs=10000,\n",
    "    callbacks=[callback],\n",
    "    validation_data=validate_ds,\n",
    "    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_model_deep5.evaluate(validate_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DD__: Lets try same as above but with stopping having more patience: __patience=1000__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_model_deep5_patience1000 = tf.keras.Sequential([\n",
    "    layers.Dense(300, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(0.001),\n",
    "                 input_shape=(FEATURES,)),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(300, activation='relu'),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(300, activation='relu'),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(300, activation='relu'),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='binary_crossentropy', patience=1000)\n",
    "\n",
    "adjusted_model_deep5_patience1000.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.05), loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.losses.BinaryCrossentropy(from_logits=True,name='binary_crossentropy'),\n",
    "                  'accuracy'], run_eagerly=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_model_deep5_patience1000.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "adjusted_model_deep5_patience1000.fit(train_ds,\n",
    "    steps_per_epoch = STEPS_PER_EPOCH,\n",
    "    epochs=10000,\n",
    "    callbacks=[callback],\n",
    "    validation_data=validate_ds,\n",
    "    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_model_deep5_patience1000.evaluate(validate_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use scikit-learn to grid search the batch size and epochs\n",
    "# import numpy\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# # Function to create model, required for KerasClassifier\n",
    "# def create_model():\n",
    "# \t# create model\n",
    "# \tmodel = Sequential()\n",
    "# \tmodel.add(Dense(12, input_dim=8, activation='relu'))\n",
    "# \tmodel.add(Dense(1, activation='sigmoid'))\n",
    "# \t# Compile model\n",
    "# \tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# \treturn model\n",
    "# # fix random seed for reproducibility\n",
    "# seed = 7\n",
    "# numpy.random.seed(seed)\n",
    "# # load dataset\n",
    "# dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# # split into input (X) and output (Y) variables\n",
    "# X = dataset[:,0:8]\n",
    "# Y = dataset[:,8]\n",
    "# # create model\n",
    "# model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "# # define the grid search parameters\n",
    "# batch_size = [10, 20, 40, 60, 80, 100]\n",
    "# epochs = [10, 50, 100]\n",
    "# param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "# grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "# grid_result = grid.fit(X, Y)\n",
    "# # summarize results\n",
    "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "# means = grid_result.cv_results_['mean_test_score']\n",
    "# stds = grid_result.cv_results_['std_test_score']\n",
    "# params = grid_result.cv_results_['params']\n",
    "# for mean, stdev, param in zip(means, stds, params):\n",
    "#     print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
