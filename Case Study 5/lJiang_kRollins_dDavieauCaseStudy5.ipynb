{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DD:__ To get ml_metrics installed I had to use Anaconda command prompt and run __pip install ml_metrics__.  I am under the impression that we should avoid using pip in an Anaconda environment but I had no choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from ml_metrics import rmse\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "\n",
    "# Convert the matrix to pandas\n",
    "bos = pd.DataFrame(boston.data)\n",
    "bos.columns = boston.feature_names\n",
    "bos['MEDV'] = boston.target\n",
    "#bos.head()\n",
    "#bos.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 1:__\n",
    "Use sklearn.datasets to get the Boston Housing dataset.  Fit a linear regressor to the data as a baseline.  There is no need to do Cross-Validation.  We will simply be exploring the change in results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = bos.sample(frac=0.7, random_state=100)\n",
    "test_set = bos[~bos.isin(train_set)].dropna()\n",
    "# Get the training and testing row indices for later use\n",
    "train_index = train_set.index.values.astype(int)\n",
    "test_index = test_set.index.values.astype(int)\n",
    "\n",
    "# Converting the training and testing datasets back to matrix-formats\n",
    "X_train = train_set.iloc[:, :-1].values # returns the data; excluding the target\n",
    "Y_train = train_set.iloc[:, -1].values # returns the target-only\n",
    "X_test = test_set.iloc[:, :-1].values # \"\"\n",
    "Y_test = test_set.iloc[:, -1].values # \"\"\n",
    "\n",
    "# Fit a linear regression to the training data\n",
    "reg = LinearRegression(normalize=True).fit(X_train, Y_train)\n",
    "\n",
    "#Predict\n",
    "Y_pred = reg.predict(X_test)\n",
    "\n",
    "#Get measures\n",
    "orig_mae = mean_absolute_error(Y_test,Y_pred)\n",
    "orig_mse = mean_squared_error(Y_test,Y_pred)\n",
    "orig_rmse_val = rmse(Y_test,Y_pred)\n",
    "orig_r2 = r2_score(Y_test,Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_frame = pd.DataFrame({'data':'original',\n",
    "                   'imputation':'none',\n",
    "                   'mae': orig_mae, \n",
    "                   'mse': orig_mse, \n",
    "                   'rmse':orig_rmse_val, \n",
    "                   'R2':orig_r2,\n",
    "                   'mae_diff':np.nan,\n",
    "                   'mse_diff':np.nan,\n",
    "                   'rmse_diff':np.nan,\n",
    "                   'R2_diff':np.nan}, index=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 1:__ What is the loss and what are the goodness of fit parameters?  This will be our baseline for comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>imputation</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>rmse</th>\n",
       "      <th>R2</th>\n",
       "      <th>mae_diff</th>\n",
       "      <th>mse_diff</th>\n",
       "      <th>rmse_diff</th>\n",
       "      <th>R2_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>original</td>\n",
       "      <td>none</td>\n",
       "      <td>3.604571</td>\n",
       "      <td>24.098505</td>\n",
       "      <td>4.909023</td>\n",
       "      <td>0.70494</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       data imputation       mae        mse      rmse       R2  mae_diff  \\\n",
       "0  original       none  3.604571  24.098505  4.909023  0.70494       NaN   \n",
       "\n",
       "   mse_diff  rmse_diff  R2_diff  \n",
       "0       NaN        NaN      NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 2:__ (repeat for each percentage value below)\n",
    "Select 1%, 5% 10%, 20%, 33%, and 50% of your data in a single column [hold that column selection constant throughout all iterations] (Completely at random), replace the original value with a NaN (i.e., “not a number” – ex., np.nan) and then perform an imputation for the missing values.   \n",
    "\n",
    "__Question 2:__ In each case [1%, 5%, 10%, 20%, 33%, 50%] perform a fit with the imputed data and compare the loss and goodness of fit to your baseline.  [Note: you should have (6) models to compare against your baseline at this point.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_sample1 = bos.sample(frac=0.01, random_state=99)\n",
    "in_sample5 = bos.sample(frac=0.05, random_state=99)\n",
    "in_sample10 = bos.sample(frac=0.1, random_state=99)\n",
    "in_sample20 = bos.sample(frac=0.2, random_state=99)\n",
    "in_sample33 = bos.sample(frac=0.33, random_state=99)\n",
    "in_sample50 = bos.sample(frac=0.50, random_state=99)\n",
    "\n",
    "out_sample1 = bos[~bos.isin(in_sample1)].dropna()\n",
    "out_sample5 = bos[~bos.isin(in_sample5)].dropna()\n",
    "out_sample10 = bos[~bos.isin(in_sample10)].dropna()\n",
    "out_sample20 = bos[~bos.isin(in_sample20)].dropna()\n",
    "out_sample33 = bos[~bos.isin(in_sample33)].dropna()\n",
    "out_sample50 = bos[~bos.isin(in_sample50)].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506\n",
      "506\n",
      "506\n",
      "506\n",
      "506\n",
      "506\n",
      "506\n"
     ]
    }
   ],
   "source": [
    "print(out_sample1.shape[0] + in_sample1.shape[0])\n",
    "print(out_sample5.shape[0] + in_sample5.shape[0])\n",
    "print(out_sample10.shape[0] + in_sample10.shape[0])\n",
    "print(out_sample20.shape[0] + in_sample20.shape[0])\n",
    "print(out_sample33.shape[0] + in_sample33.shape[0])\n",
    "print(out_sample50.shape[0] + in_sample50.shape[0])\n",
    "print(bos.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_sample1['DIS'] = np.nan\n",
    "in_sample5['DIS'] = np.nan\n",
    "in_sample10['DIS'] = np.nan\n",
    "in_sample20['DIS'] = np.nan\n",
    "in_sample33['DIS'] = np.nan\n",
    "in_sample50['DIS'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choose an imputation method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_sample1['DIS'] = in_sample1['DIS'].fillna(out_sample1['DIS'].mean())\n",
    "in_sample5['DIS'] = in_sample5['DIS'].fillna(out_sample5['DIS'].mean())\n",
    "in_sample10['DIS'] = in_sample10['DIS'].fillna(out_sample10['DIS'].mean())\n",
    "in_sample20['DIS'] = in_sample20['DIS'].fillna(out_sample20['DIS'].mean())\n",
    "in_sample33['DIS'] = in_sample33['DIS'].fillna(out_sample33['DIS'].mean())\n",
    "in_sample50['DIS'] = in_sample50['DIS'].fillna(out_sample50['DIS'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rejoin the imputed and original datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data1 = pd.concat([in_sample1, out_sample1])\n",
    "imputed_data1 = imputed_data1.sort_index()\n",
    "\n",
    "imputed_data5 = pd.concat([in_sample5, out_sample5])\n",
    "imputed_data5 = imputed_data5.sort_index()\n",
    "\n",
    "imputed_data10 = pd.concat([in_sample10, out_sample10])\n",
    "imputed_data10 = imputed_data10.sort_index()\n",
    "\n",
    "imputed_data20 = pd.concat([in_sample20, out_sample20])\n",
    "imputed_data20 = imputed_data20.sort_index()\n",
    "\n",
    "imputed_data33 = pd.concat([in_sample33, out_sample33])\n",
    "imputed_data33 = imputed_data33.sort_index()\n",
    "\n",
    "imputed_data50 = pd.concat([in_sample50, out_sample50])\n",
    "imputed_data50 = imputed_data50.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use the same training and testing indices to fit the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set1 = imputed_data1.iloc[train_index]\n",
    "test_set1 = imputed_data1.iloc[test_index]\n",
    "\n",
    "train_set5 = imputed_data5.iloc[train_index]\n",
    "test_set5 = imputed_data5.iloc[test_index]\n",
    "\n",
    "train_set10 = imputed_data10.iloc[train_index]\n",
    "test_set10 = imputed_data10.iloc[test_index]\n",
    "\n",
    "train_set20 = imputed_data20.iloc[train_index]\n",
    "test_set20 = imputed_data20.iloc[test_index]\n",
    "\n",
    "train_set33 = imputed_data33.iloc[train_index]\n",
    "test_set33 = imputed_data33.iloc[test_index]\n",
    "\n",
    "train_set50 = imputed_data50.iloc[train_index]\n",
    "test_set50 = imputed_data50.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = train_set1.iloc[:, :-1].values\n",
    "Y_train1 = train_set1.iloc[:, -1].values\n",
    "X_test1 = test_set1.iloc[:, :-1].values\n",
    "Y_test1 = test_set1.iloc[:, -1].values\n",
    "\n",
    "X_train5 = train_set5.iloc[:, :-1].values\n",
    "Y_train5 = train_set5.iloc[:, -1].values\n",
    "X_test5 = test_set5.iloc[:, :-1].values\n",
    "Y_test5 = test_set5.iloc[:, -1].values\n",
    "\n",
    "X_train10 = train_set10.iloc[:, :-1].values\n",
    "Y_train10 = train_set10.iloc[:, -1].values\n",
    "X_test10 = test_set10.iloc[:, :-1].values\n",
    "Y_test10 = test_set10.iloc[:, -1].values\n",
    "\n",
    "X_train20 = train_set20.iloc[:, :-1].values\n",
    "Y_train20 = train_set20.iloc[:, -1].values\n",
    "X_test20 = test_set20.iloc[:, :-1].values\n",
    "Y_test20 = test_set20.iloc[:, -1].values\n",
    "\n",
    "X_train33 = train_set33.iloc[:, :-1].values\n",
    "Y_train33 = train_set33.iloc[:, -1].values\n",
    "X_test33 = test_set33.iloc[:, :-1].values\n",
    "Y_test33 = test_set33.iloc[:, -1].values\n",
    "\n",
    "X_train50 = train_set50.iloc[:, :-1].values\n",
    "Y_train50 = train_set50.iloc[:, -1].values\n",
    "X_test50 = test_set50.iloc[:, :-1].values\n",
    "Y_test50 = test_set50.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit a new model to the imputed dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg1 = LinearRegression().fit(X_train1, Y_train1)\n",
    "reg5 = LinearRegression().fit(X_train5, Y_train5)\n",
    "reg10 = LinearRegression().fit(X_train10, Y_train10)\n",
    "reg20 = LinearRegression().fit(X_train20, Y_train20)\n",
    "reg33 = LinearRegression().fit(X_train33, Y_train33)\n",
    "reg50 = LinearRegression().fit(X_train50, Y_train50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred1 = reg1.predict(X_test1)\n",
    "Y_pred5 = reg5.predict(X_test5)\n",
    "Y_pred10 = reg10.predict(X_test10)\n",
    "Y_pred20 = reg20.predict(X_test20)\n",
    "Y_pred33 = reg33.predict(X_test33)\n",
    "Y_pred50 = reg50.predict(X_test50)\n",
    "\n",
    "mae1 = mean_absolute_error(Y_test1,Y_pred1)\n",
    "mae5 = mean_absolute_error(Y_test5,Y_pred5)\n",
    "mae10 = mean_absolute_error(Y_test10,Y_pred10)\n",
    "mae20 = mean_absolute_error(Y_test20,Y_pred20)\n",
    "mae33 = mean_absolute_error(Y_test33,Y_pred33)\n",
    "mae50 = mean_absolute_error(Y_test50,Y_pred50)\n",
    "\n",
    "mse1 = mean_squared_error(Y_test1,Y_pred1)\n",
    "mse5 = mean_squared_error(Y_test5,Y_pred5)\n",
    "mse10 = mean_squared_error(Y_test10,Y_pred10)\n",
    "mse20 = mean_squared_error(Y_test20,Y_pred20)\n",
    "mse33 = mean_squared_error(Y_test33,Y_pred33)\n",
    "mse50 = mean_squared_error(Y_test50,Y_pred50)\n",
    "\n",
    "rmse_val1 = rmse(Y_test1,Y_pred1)\n",
    "rmse_val5 = rmse(Y_test5,Y_pred5)\n",
    "rmse_val10 = rmse(Y_test10,Y_pred10)\n",
    "rmse_val20 = rmse(Y_test20,Y_pred20)\n",
    "rmse_val33 = rmse(Y_test33,Y_pred33)\n",
    "rmse_val50 = rmse(Y_test50,Y_pred50)\n",
    "\n",
    "r21 = r2_score(Y_test1,Y_pred1)\n",
    "r25 = r2_score(Y_test5,Y_pred5)\n",
    "r210 = r2_score(Y_test10,Y_pred10)\n",
    "r220 = r2_score(Y_test20,Y_pred20)\n",
    "r233 = r2_score(Y_test33,Y_pred33)\n",
    "r250 = r2_score(Y_test50,Y_pred50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DD: Left off here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_frame = pd.DataFrame({'data':'1% imputed',\n",
    "                   'imputation':'Mean',\n",
    "                   'mae': mae, \n",
    "                   'mse': mse, \n",
    "                   'rmse':rmse_val,\n",
    "                   'R2':r2,\n",
    "                   'mae_diff':mae-orig_mae,\n",
    "                   'mse_diff':mse-orig_mse,\n",
    "                   'rmse_diff':rmse_val-orig_rmse_val,\n",
    "                   'R2_diff':r2-orig_r2\n",
    "                   }, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_frame = pd.concat([res_frame, temp_frame])\n",
    "res_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 3:__ Take two columns and create data “Missing at Random” when controlled for a third variable (i.e., if Variable Z is > 30, then Variables X, Y are randomly missing).  Use your preferred imputation method to fill in 10%, 20% and 30% of your missing data.\n",
    "\n",
    "__Question 3:__ In each case [10%, 20%, 30%] perform a fit with the imputed data and compare the loss and goodness of fit to your baseline.  [Note: you should have (9) models to compare against your baseline at this point.]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 4:__  Create a “Missing Not at Random” pattern in which 25% of the data is missing for a single column.\n",
    "\n",
    "__Question 4:__ Perform a fit with the imputed data [25%] and compare the loss and goodness of fit to your baseline.  [Note: you should have (10) models to compare against your baseline at this point.]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 5:__ Describe your imputation approach and summarize your findings.  What impact did the missing data have on your baseline model’s performance? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
