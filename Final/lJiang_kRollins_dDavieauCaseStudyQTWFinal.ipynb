{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qtw final\n",
    "\n",
    "Required\n",
    "Deliverable:\n",
    "I want you to minimize my dollar cost on an UNKNOWN dataset\n",
    "That means one you can never see.  Ever.\n",
    "Each False Positive costs me $10\n",
    "Each False Negative costs me $500\n",
    "True Positives and True Negatives cost me $0\n",
    "\n",
    "\n",
    "Problem:\n",
    "Cost-sensitive learning \n",
    "\n",
    "cost function:\n",
    " Total Cost = 10 * Count of FP + 500 * Count of FN\n",
    "\n",
    "Goal: minimize False Negative or Type-II error\n",
    "\n",
    "\n",
    "Precision = TruePositives / (TruePositives + FalsePositives)\n",
    "\n",
    "Recall = TruePositives / (TruePositives + FalseNegatives)\n",
    "\n",
    "\n",
    "The intuition for recall is that it is not concerned with false positives and it minimizes false negatives. \n",
    "\n",
    "Precision is a metric that calculates the percentage of correct predictions for the positive class. Recall calculates the percentage of correct predictions for the positive class out of all positive predictions that could be made. Maximizing precision will minimize the false-positive errors, whereas maximizing recall will minimize the false-negative errors.\n",
    "\n",
    "\n",
    "Methods\n",
    "Metric Choice\n",
    "\n",
    "\n",
    " Fbeta \n",
    "\n",
    "Fbeta = ((1 + beta^2) * Precision * Recall) / (beta^2 * Precision + Recall)\n",
    "\n",
    "When choosing beta in your F-beta score the more you care about recall over precision the higher beta you should choose. For example, with F1 score we care equally about recall and precision with F2 score, recall is twice as important to us.\n",
    "\n",
    "\n",
    "F50\n",
    "\n",
    "\n",
    "How to compute:\n",
    "from sklearn.metrics import fbeta_score\n",
    "y_pred_class = y_pred_pos > threshold\n",
    "fbeta_score(y_true, y_pred_class, beta)\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html\n",
    "\n",
    "\n",
    " calculate the f2-measure\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "# perfect precision, 50% recall\n",
    "y_true = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
    "y_pred = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "p = precision_score(y_true, y_pred)\n",
    "r = recall_score(y_true, y_pred)\n",
    "f = fbeta_score(y_true, y_pred, beta=2.0)\n",
    "print('Result: p=%.3f, r=%.3f, f=%.3f' % (p, r, f))\n",
    "\n",
    "\n",
    "\n",
    "eda\n",
    "\n",
    "\n",
    "check y distribution\n",
    "\n",
    "pca and see\n",
    "\n",
    "xgb f beta\n",
    "if have time rf\n",
    "\n",
    "ref\n",
    "https://machinelearningmastery.com/fbeta-measure-for-machine-learning/\n",
    "https://stackoverflow.com/questions/41060719/using-evaluation-functions-with-parameter-in-xgboost-f-beta\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
